{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef1927/dl-samples/blob/master/linear-regression/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "This is a notebook for reviewing the basics of linear regression and how it can be implemented from first principles in numpy and pytorch.\n",
        "\n",
        "Sources:\n",
        "\n",
        "* https://www.kaggle.com/aakashns/pytorch-basics-linear-regression-from-scratch\n",
        "* https://nbviewer.org/url/www.cs.toronto.edu/~frossard/post/linear_regression/Linear%20Regression.ipynb"
      ],
      "metadata": {
        "id": "Yh9FKE1vifxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PN47opL0gVCO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the data"
      ],
      "metadata": {
        "id": "_DRfvaWQiyVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs_orig = np.array([[73, 67, 43], \n",
        "                        [91, 88, 64], \n",
        "                        [87, 134, 58], \n",
        "                        [102, 43, 37], \n",
        "                        [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets_orig = np.array([[56, 70], \n",
        "                         [81, 101], \n",
        "                         [119, 133], \n",
        "                         [22, 37], \n",
        "                         [103, 119]], dtype='float32')"
      ],
      "metadata": {
        "id": "OJ8YfL3Ki4gV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will fit crop yields for apples and oranges by looking at temperature, rainfall and humidity.\n",
        "\n",
        "| Region  | Temp (F) | Rainfall (mm) | Humidity (%) | Apples (ton) | Oranges (ton) |\n",
        "| ---   | --- | ----| ---| --- | --- |\n",
        "|Kanto  | 73  | 67  | 43 | 56  | 70  |\n",
        "|Johto  | 91  | 88  | 64 | 81  | 101 |\n",
        "|Hoenn  | 87  | 134 | 58 | 119 | 133 |\n",
        "|Sinnoh | 102 | 43  | 37 | 22  | 37  |\n",
        "|Unova  | 69  | 96  | 70 | 103 | 119 | \n",
        "\n",
        "Each target will be estimated as a weighted sum of the input variables, offset by the bias, where the bias and the weights will be learnt through gradient descent.\n",
        "\n",
        "$y_{apples} = w_{11}*temp + w_{21} * rainfall + w_{13} * humidity + b_1$\n",
        "\n",
        "$y_{oranges} = w_{21}*temp + w_{22} * rainfall + w_{23} * humidity + b_2$\n"
      ],
      "metadata": {
        "id": "bWO0Y8ukk_hC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch from scratch"
      ],
      "metadata": {
        "id": "GtB6QnWypley"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-5\n",
        "epochs = 1000\n",
        "inputs = torch.tensor(inputs_orig, requires_grad=True)\n",
        "targets = torch.tensor(targets_orig, requires_grad=True)\n",
        "\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)\n",
        "\n",
        "def model(x):\n",
        "  return x @ w.t() + b\n",
        "\n",
        "def mse(t1, t2):\n",
        "  diff = t1 - t2\n",
        "  return torch.sum(diff * diff) / diff.numel()\n",
        "\n",
        "for i in range(epochs):\n",
        "  #print('i: {} W: {}/{}, b: {}/{}'.format(i, w, w.grad, b, b.grad))\n",
        "  \n",
        "  # compute predictions and loss\n",
        "  preds = model(inputs)\n",
        "  loss = mse(preds, targets)\n",
        "\n",
        "  # compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # adjust the weights and reset the gradients\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "    b -= lr * b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  \n",
        "  if i > 0 and i % 10 == 0:\n",
        "    print('***Epoch: {}, Loss:{}***'.format(i, loss))\n",
        "  \n",
        "print('***Final***\\nTargets: {}\\nPredictions:{}\\nLoss:{}\\n'.format(targets, preds, loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIK9dNI0pxuG",
        "outputId": "4398e249-a834-453f-e923-4332f0498ae5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Epoch: 10, Loss:3237.90576171875***\n",
            "***Epoch: 20, Loss:2319.900146484375***\n",
            "***Epoch: 30, Loss:2046.9105224609375***\n",
            "***Epoch: 40, Loss:1816.579833984375***\n",
            "***Epoch: 50, Loss:1613.6754150390625***\n",
            "***Epoch: 60, Loss:1434.7327880859375***\n",
            "***Epoch: 70, Loss:1276.893798828125***\n",
            "***Epoch: 80, Loss:1137.6444091796875***\n",
            "***Epoch: 90, Loss:1014.7706298828125***\n",
            "***Epoch: 100, Loss:906.3230590820312***\n",
            "***Epoch: 110, Loss:810.5841064453125***\n",
            "***Epoch: 120, Loss:726.0416259765625***\n",
            "***Epoch: 130, Loss:651.3638916015625***\n",
            "***Epoch: 140, Loss:585.3777465820312***\n",
            "***Epoch: 150, Loss:527.0499267578125***\n",
            "***Epoch: 160, Loss:475.47052001953125***\n",
            "***Epoch: 170, Loss:429.8382263183594***\n",
            "***Epoch: 180, Loss:389.447265625***\n",
            "***Epoch: 190, Loss:353.6757507324219***\n",
            "***Epoch: 200, Loss:321.97607421875***\n",
            "***Epoch: 210, Loss:293.86627197265625***\n",
            "***Epoch: 220, Loss:268.92132568359375***\n",
            "***Epoch: 230, Loss:246.76693725585938***\n",
            "***Epoch: 240, Loss:227.0738067626953***\n",
            "***Epoch: 250, Loss:209.55126953125***\n",
            "***Epoch: 260, Loss:193.94400024414062***\n",
            "***Epoch: 270, Loss:180.0264892578125***\n",
            "***Epoch: 280, Loss:167.60023498535156***\n",
            "***Epoch: 290, Loss:156.4904022216797***\n",
            "***Epoch: 300, Loss:146.54312133789062***\n",
            "***Epoch: 310, Loss:137.62257385253906***\n",
            "***Epoch: 320, Loss:129.6091766357422***\n",
            "***Epoch: 330, Loss:122.39781188964844***\n",
            "***Epoch: 340, Loss:115.89527893066406***\n",
            "***Epoch: 350, Loss:110.02006530761719***\n",
            "***Epoch: 360, Loss:104.69989013671875***\n",
            "***Epoch: 370, Loss:99.87138366699219***\n",
            "***Epoch: 380, Loss:95.47825622558594***\n",
            "***Epoch: 390, Loss:91.47152709960938***\n",
            "***Epoch: 400, Loss:87.807373046875***\n",
            "***Epoch: 410, Loss:84.4474868774414***\n",
            "***Epoch: 420, Loss:81.35797119140625***\n",
            "***Epoch: 430, Loss:78.509033203125***\n",
            "***Epoch: 440, Loss:75.87435150146484***\n",
            "***Epoch: 450, Loss:73.43073272705078***\n",
            "***Epoch: 460, Loss:71.15754699707031***\n",
            "***Epoch: 470, Loss:69.0367660522461***\n",
            "***Epoch: 480, Loss:67.05233001708984***\n",
            "***Epoch: 490, Loss:65.19014739990234***\n",
            "***Epoch: 500, Loss:63.437705993652344***\n",
            "***Epoch: 510, Loss:61.784217834472656***\n",
            "***Epoch: 520, Loss:60.219627380371094***\n",
            "***Epoch: 530, Loss:58.73554229736328***\n",
            "***Epoch: 540, Loss:57.32416534423828***\n",
            "***Epoch: 550, Loss:55.97895431518555***\n",
            "***Epoch: 560, Loss:54.69364547729492***\n",
            "***Epoch: 570, Loss:53.463043212890625***\n",
            "***Epoch: 580, Loss:52.282569885253906***\n",
            "***Epoch: 590, Loss:51.14796447753906***\n",
            "***Epoch: 600, Loss:50.055450439453125***\n",
            "***Epoch: 610, Loss:49.001808166503906***\n",
            "***Epoch: 620, Loss:47.98405838012695***\n",
            "***Epoch: 630, Loss:46.9995231628418***\n",
            "***Epoch: 640, Loss:46.04597091674805***\n",
            "***Epoch: 650, Loss:45.121307373046875***\n",
            "***Epoch: 660, Loss:44.22356414794922***\n",
            "***Epoch: 670, Loss:43.35110092163086***\n",
            "***Epoch: 680, Loss:42.50243377685547***\n",
            "***Epoch: 690, Loss:41.676246643066406***\n",
            "***Epoch: 700, Loss:40.87123489379883***\n",
            "***Epoch: 710, Loss:40.08639907836914***\n",
            "***Epoch: 720, Loss:39.32066345214844***\n",
            "***Epoch: 730, Loss:38.573143005371094***\n",
            "***Epoch: 740, Loss:37.84304428100586***\n",
            "***Epoch: 750, Loss:37.129573822021484***\n",
            "***Epoch: 760, Loss:36.43206787109375***\n",
            "***Epoch: 770, Loss:35.749942779541016***\n",
            "***Epoch: 780, Loss:35.08253479003906***\n",
            "***Epoch: 790, Loss:34.429405212402344***\n",
            "***Epoch: 800, Loss:33.78997039794922***\n",
            "***Epoch: 810, Loss:33.16393280029297***\n",
            "***Epoch: 820, Loss:32.55072784423828***\n",
            "***Epoch: 830, Loss:31.949975967407227***\n",
            "***Epoch: 840, Loss:31.361364364624023***\n",
            "***Epoch: 850, Loss:30.78452491760254***\n",
            "***Epoch: 860, Loss:30.21917152404785***\n",
            "***Epoch: 870, Loss:29.664871215820312***\n",
            "***Epoch: 880, Loss:29.12149429321289***\n",
            "***Epoch: 890, Loss:28.588735580444336***\n",
            "***Epoch: 900, Loss:28.06625747680664***\n",
            "***Epoch: 910, Loss:27.55388832092285***\n",
            "***Epoch: 920, Loss:27.051321029663086***\n",
            "***Epoch: 930, Loss:26.558399200439453***\n",
            "***Epoch: 940, Loss:26.074853897094727***\n",
            "***Epoch: 950, Loss:25.600540161132812***\n",
            "***Epoch: 960, Loss:25.13520050048828***\n",
            "***Epoch: 970, Loss:24.678653717041016***\n",
            "***Epoch: 980, Loss:24.230743408203125***\n",
            "***Epoch: 990, Loss:23.791259765625***\n",
            "***Final***\n",
            "Targets: tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]], requires_grad=True)\n",
            "Predictions:tensor([[ 57.3150,  70.9339],\n",
            "        [ 82.7986,  95.9276],\n",
            "        [117.1101, 142.7410],\n",
            "        [ 20.8771,  39.8610],\n",
            "        [103.2972, 109.2805]], grad_fn=<AddBackward0>)\n",
            "Loss:23.402820587158203\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch built-ins"
      ],
      "metadata": {
        "id": "moU0Cw5Avk3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "lr = 1e-5\n",
        "epochs = 1000\n",
        "batch_size = 5\n",
        "inputs = torch.tensor(inputs_orig, requires_grad=True)\n",
        "targets = torch.tensor(targets_orig, requires_grad=True)\n",
        "\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "\n",
        "model = nn.Linear(3, 2)\n",
        "print(model.weight)\n",
        "print(model.bias)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fcn = F.mse_loss\n",
        "\n",
        "for i in range(epochs):\n",
        "  for xb, yb in train_dl:\n",
        "    # compute predictions and loss\n",
        "    preds = model(xb)\n",
        "    loss = loss_fcn(preds, yb)\n",
        "\n",
        "    # compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # adjust the weights and reset the gradients\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "  if i > 0 and i % 10 == 0:\n",
        "    print('***Epoch: {}, Loss:{}***'.format(i, loss))\n",
        "  \n",
        "print('***Final***\\nTargets: {}\\nPredictions:{}\\nLoss:{}\\n'.format(targets, preds, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UGan959vp0Z",
        "outputId": "4891a593-7db3-4948-94b0-09ab0c2da932"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3124,  0.5762,  0.0886],\n",
            "        [ 0.1606, -0.0734, -0.3997]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.2629, -0.1043], requires_grad=True)\n",
            "***Epoch: 10, Loss:630.4852905273438***\n",
            "***Epoch: 20, Loss:458.3590393066406***\n",
            "***Epoch: 30, Loss:407.7231750488281***\n",
            "***Epoch: 40, Loss:364.97100830078125***\n",
            "***Epoch: 50, Loss:327.24951171875***\n",
            "***Epoch: 60, Loss:293.92340087890625***\n",
            "***Epoch: 70, Loss:264.4695739746094***\n",
            "***Epoch: 80, Loss:238.4278106689453***\n",
            "***Epoch: 90, Loss:215.39279174804688***\n",
            "***Epoch: 100, Loss:195.007568359375***\n",
            "***Epoch: 110, Loss:176.957763671875***\n",
            "***Epoch: 120, Loss:160.9663848876953***\n",
            "***Epoch: 130, Loss:146.78964233398438***\n",
            "***Epoch: 140, Loss:134.21275329589844***\n",
            "***Epoch: 150, Loss:123.04647064208984***\n",
            "***Epoch: 160, Loss:113.12413024902344***\n",
            "***Epoch: 170, Loss:104.29890441894531***\n",
            "***Epoch: 180, Loss:96.44148254394531***\n",
            "***Epoch: 190, Loss:89.43794250488281***\n",
            "***Epoch: 200, Loss:83.18792724609375***\n",
            "***Epoch: 210, Loss:77.6031265258789***\n",
            "***Epoch: 220, Loss:72.60551452636719***\n",
            "***Epoch: 230, Loss:68.12659454345703***\n",
            "***Epoch: 240, Loss:64.10585021972656***\n",
            "***Epoch: 250, Loss:60.48999786376953***\n",
            "***Epoch: 260, Loss:57.2321662902832***\n",
            "***Epoch: 270, Loss:54.290924072265625***\n",
            "***Epoch: 280, Loss:51.629905700683594***\n",
            "***Epoch: 290, Loss:49.216896057128906***\n",
            "***Epoch: 300, Loss:47.02363967895508***\n",
            "***Epoch: 310, Loss:45.02516555786133***\n",
            "***Epoch: 320, Loss:43.199424743652344***\n",
            "***Epoch: 330, Loss:41.527069091796875***\n",
            "***Epoch: 340, Loss:39.99104690551758***\n",
            "***Epoch: 350, Loss:38.576107025146484***\n",
            "***Epoch: 360, Loss:37.2690544128418***\n",
            "***Epoch: 370, Loss:36.05813980102539***\n",
            "***Epoch: 380, Loss:34.93299102783203***\n",
            "***Epoch: 390, Loss:33.884437561035156***\n",
            "***Epoch: 400, Loss:32.90438461303711***\n",
            "***Epoch: 410, Loss:31.985733032226562***\n",
            "***Epoch: 420, Loss:31.122203826904297***\n",
            "***Epoch: 430, Loss:30.3082218170166***\n",
            "***Epoch: 440, Loss:29.53885841369629***\n",
            "***Epoch: 450, Loss:28.809717178344727***\n",
            "***Epoch: 460, Loss:28.117055892944336***\n",
            "***Epoch: 470, Loss:27.45736312866211***\n",
            "***Epoch: 480, Loss:26.82761001586914***\n",
            "***Epoch: 490, Loss:26.225173950195312***\n",
            "***Epoch: 500, Loss:25.647724151611328***\n",
            "***Epoch: 510, Loss:25.09311294555664***\n",
            "***Epoch: 520, Loss:24.55938148498535***\n",
            "***Epoch: 530, Loss:24.044994354248047***\n",
            "***Epoch: 540, Loss:23.54842758178711***\n",
            "***Epoch: 550, Loss:23.06830406188965***\n",
            "***Epoch: 560, Loss:22.603530883789062***\n",
            "***Epoch: 570, Loss:22.152997970581055***\n",
            "***Epoch: 580, Loss:21.71584701538086***\n",
            "***Epoch: 590, Loss:21.291141510009766***\n",
            "***Epoch: 600, Loss:20.87814712524414***\n",
            "***Epoch: 610, Loss:20.476224899291992***\n",
            "***Epoch: 620, Loss:20.084697723388672***\n",
            "***Epoch: 630, Loss:19.703094482421875***\n",
            "***Epoch: 640, Loss:19.33089256286621***\n",
            "***Epoch: 650, Loss:18.967615127563477***\n",
            "***Epoch: 660, Loss:18.612882614135742***\n",
            "***Epoch: 670, Loss:18.26624298095703***\n",
            "***Epoch: 680, Loss:17.927480697631836***\n",
            "***Epoch: 690, Loss:17.596195220947266***\n",
            "***Epoch: 700, Loss:17.27212905883789***\n",
            "***Epoch: 710, Loss:16.954998016357422***\n",
            "***Epoch: 720, Loss:16.644607543945312***\n",
            "***Epoch: 730, Loss:16.340675354003906***\n",
            "***Epoch: 740, Loss:16.043048858642578***\n",
            "***Epoch: 750, Loss:15.751492500305176***\n",
            "***Epoch: 760, Loss:15.465800285339355***\n",
            "***Epoch: 770, Loss:15.185884475708008***\n",
            "***Epoch: 780, Loss:14.911540031433105***\n",
            "***Epoch: 790, Loss:14.642565727233887***\n",
            "***Epoch: 800, Loss:14.378924369812012***\n",
            "***Epoch: 810, Loss:14.120448112487793***\n",
            "***Epoch: 820, Loss:13.86694049835205***\n",
            "***Epoch: 830, Loss:13.618356704711914***\n",
            "***Epoch: 840, Loss:13.3745698928833***\n",
            "***Epoch: 850, Loss:13.13544750213623***\n",
            "***Epoch: 860, Loss:12.900860786437988***\n",
            "***Epoch: 870, Loss:12.670781135559082***\n",
            "***Epoch: 880, Loss:12.445039749145508***\n",
            "***Epoch: 890, Loss:12.223584175109863***\n",
            "***Epoch: 900, Loss:12.006332397460938***\n",
            "***Epoch: 910, Loss:11.793152809143066***\n",
            "***Epoch: 920, Loss:11.583983421325684***\n",
            "***Epoch: 930, Loss:11.37874698638916***\n",
            "***Epoch: 940, Loss:11.177352905273438***\n",
            "***Epoch: 950, Loss:10.979730606079102***\n",
            "***Epoch: 960, Loss:10.785853385925293***\n",
            "***Epoch: 970, Loss:10.595540046691895***\n",
            "***Epoch: 980, Loss:10.408794403076172***\n",
            "***Epoch: 990, Loss:10.225509643554688***\n",
            "***Final***\n",
            "Targets: tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]], requires_grad=True)\n",
            "Predictions:tensor([[ 98.5525, 113.6319],\n",
            "        [ 57.4111,  70.6195],\n",
            "        [ 22.0777,  38.6345],\n",
            "        [122.0083, 138.3956],\n",
            "        [ 80.5757,  98.0599]], grad_fn=<AddmmBackward0>)\n",
            "Loss:10.063506126403809\n",
            "\n"
          ]
        }
      ]
    }
  ]
}