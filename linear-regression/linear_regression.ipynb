{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "This is a notebook for reviewing the basics of linear regression and how it can be implemented from first principles in numpy and pytorch.\n",
        "\n",
        "Sources:\n",
        "\n",
        "* https://www.kaggle.com/aakashns/pytorch-basics-linear-regression-from-scratch\n",
        "* https://nbviewer.org/url/www.cs.toronto.edu/~frossard/post/linear_regression/Linear%20Regression.ipynb"
      ],
      "metadata": {
        "id": "Yh9FKE1vifxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN47opL0gVCO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the data"
      ],
      "metadata": {
        "id": "_DRfvaWQiyVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs_orig = np.array([[73, 67, 43], \n",
        "                        [91, 88, 64], \n",
        "                        [87, 134, 58], \n",
        "                        [102, 43, 37], \n",
        "                        [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets_orig = np.array([[56, 70], \n",
        "                         [81, 101], \n",
        "                         [119, 133], \n",
        "                         [22, 37], \n",
        "                         [103, 119]], dtype='float32')"
      ],
      "metadata": {
        "id": "OJ8YfL3Ki4gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will fit crop yields for apples and oranges by looking at temperature, rainfall and humidity.\n",
        "\n",
        "| Region  | Temp (F) | Rainfall (mm) | Humidity (%) | Apples (ton) | Oranges (ton) |\n",
        "| ---   | --- | ----| ---| --- | --- |\n",
        "|Kanto  | 73  | 67  | 43 | 56  | 70  |\n",
        "|Johto  | 91  | 88  | 64 | 81  | 101 |\n",
        "|Hoenn  | 87  | 134 | 58 | 119 | 133 |\n",
        "|Sinnoh | 102 | 43  | 37 | 22  | 37  |\n",
        "|Unova  | 69  | 96  | 70 | 103 | 119 | \n",
        "\n",
        "Each target will be estimated as a weighted sum of the input variables, offset by the bias, where the bias and the weights will be learnt through gradient descent.\n",
        "\n",
        "$y_{apples} = w_{11}*temp + w_{21} * rainfall + w_{13} * humidity + b_1$\n",
        "\n",
        "$y_{oranges} = w_{21}*temp + w_{22} * rainfall + w_{23} * humidity + b_2$\n"
      ],
      "metadata": {
        "id": "bWO0Y8ukk_hC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch from scratch"
      ],
      "metadata": {
        "id": "GtB6QnWypley"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-5\n",
        "epochs = 1000\n",
        "inputs = torch.tensor(inputs_orig, requires_grad=True)\n",
        "targets = torch.tensor(targets_orig, requires_grad=True)\n",
        "\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)\n",
        "\n",
        "def model(x):\n",
        "  return x @ w.t() + b\n",
        "\n",
        "def mse(t1, t2):\n",
        "  diff = t1 - t2\n",
        "  return torch.sum(diff * diff) / diff.numel()\n",
        "\n",
        "for i in range(epochs):\n",
        "  #print('i: {} W: {}/{}, b: {}/{}'.format(i, w, w.grad, b, b.grad))\n",
        "  \n",
        "  # compute predictions and loss\n",
        "  preds = model(inputs)\n",
        "  loss = mse(preds, targets)\n",
        "\n",
        "  # compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # adjust the weights and reset the gradients\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "    b -= lr * b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  \n",
        "  if i > 0 and i % 10 == 0:\n",
        "    print('***Epoch: {}, Loss:{}***'.format(i, loss))\n",
        "  \n",
        "print('***Final***\\nTargets: {}\\nPredictions:{}\\nLoss:{}\\n'.format(targets, preds, loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIK9dNI0pxuG",
        "outputId": "021a8ab7-7fd6-4d0a-bc55-606ec56680ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Epoch: 10, Loss:5526.90966796875***\n",
            "***Epoch: 20, Loss:4034.72705078125***\n",
            "***Epoch: 30, Loss:3538.3203125***\n",
            "***Epoch: 40, Loss:3116.92626953125***\n",
            "***Epoch: 50, Loss:2746.104736328125***\n",
            "***Epoch: 60, Loss:2419.520751953125***\n",
            "***Epoch: 70, Loss:2131.89111328125***\n",
            "***Epoch: 80, Loss:1878.5670166015625***\n",
            "***Epoch: 90, Loss:1655.4547119140625***\n",
            "***Epoch: 100, Loss:1458.94921875***\n",
            "***Epoch: 110, Loss:1285.875732421875***\n",
            "***Epoch: 120, Loss:1133.4384765625***\n",
            "***Epoch: 130, Loss:999.1751098632812***\n",
            "***Epoch: 140, Loss:880.9169921875***\n",
            "***Epoch: 150, Loss:776.7545776367188***\n",
            "***Epoch: 160, Loss:685.005859375***\n",
            "***Epoch: 170, Loss:604.1898803710938***\n",
            "***Epoch: 180, Loss:533.0022583007812***\n",
            "***Epoch: 190, Loss:470.2943420410156***\n",
            "***Epoch: 200, Loss:415.0543518066406***\n",
            "***Epoch: 210, Loss:366.3914489746094***\n",
            "***Epoch: 220, Loss:323.52093505859375***\n",
            "***Epoch: 230, Loss:285.751953125***\n",
            "***Epoch: 240, Loss:252.4760284423828***\n",
            "***Epoch: 250, Loss:223.1570587158203***\n",
            "***Epoch: 260, Loss:197.32321166992188***\n",
            "***Epoch: 270, Loss:174.55880737304688***\n",
            "***Epoch: 280, Loss:154.49781799316406***\n",
            "***Epoch: 290, Loss:136.8179168701172***\n",
            "***Epoch: 300, Loss:121.23503112792969***\n",
            "***Epoch: 310, Loss:107.49930572509766***\n",
            "***Epoch: 320, Loss:95.3904800415039***\n",
            "***Epoch: 330, Loss:84.71463775634766***\n",
            "***Epoch: 340, Loss:75.30101013183594***\n",
            "***Epoch: 350, Loss:66.9991455078125***\n",
            "***Epoch: 360, Loss:59.6766471862793***\n",
            "***Epoch: 370, Loss:53.216819763183594***\n",
            "***Epoch: 380, Loss:47.51691818237305***\n",
            "***Epoch: 390, Loss:42.48645782470703***\n",
            "***Epoch: 400, Loss:38.045753479003906***\n",
            "***Epoch: 410, Loss:34.124603271484375***\n",
            "***Epoch: 420, Loss:30.66123390197754***\n",
            "***Epoch: 430, Loss:27.601177215576172***\n",
            "***Epoch: 440, Loss:24.896472930908203***\n",
            "***Epoch: 450, Loss:22.504955291748047***\n",
            "***Epoch: 460, Loss:20.389387130737305***\n",
            "***Epoch: 470, Loss:18.517017364501953***\n",
            "***Epoch: 480, Loss:16.858945846557617***\n",
            "***Epoch: 490, Loss:15.389854431152344***\n",
            "***Epoch: 500, Loss:14.087305068969727***\n",
            "***Epoch: 510, Loss:12.931622505187988***\n",
            "***Epoch: 520, Loss:11.90538215637207***\n",
            "***Epoch: 530, Loss:10.993322372436523***\n",
            "***Epoch: 540, Loss:10.181986808776855***\n",
            "***Epoch: 550, Loss:9.459495544433594***\n",
            "***Epoch: 560, Loss:8.815393447875977***\n",
            "***Epoch: 570, Loss:8.240459442138672***\n",
            "***Epoch: 580, Loss:7.726602077484131***\n",
            "***Epoch: 590, Loss:7.266640663146973***\n",
            "***Epoch: 600, Loss:6.854316711425781***\n",
            "***Epoch: 610, Loss:6.484041690826416***\n",
            "***Epoch: 620, Loss:6.150966644287109***\n",
            "***Epoch: 630, Loss:5.850766181945801***\n",
            "***Epoch: 640, Loss:5.5796403884887695***\n",
            "***Epoch: 650, Loss:5.334262371063232***\n",
            "***Epoch: 660, Loss:5.111649513244629***\n",
            "***Epoch: 670, Loss:4.909214973449707***\n",
            "***Epoch: 680, Loss:4.72470235824585***\n",
            "***Epoch: 690, Loss:4.55605411529541***\n",
            "***Epoch: 700, Loss:4.401501178741455***\n",
            "***Epoch: 710, Loss:4.259483337402344***\n",
            "***Epoch: 720, Loss:4.128581523895264***\n",
            "***Epoch: 730, Loss:4.007623195648193***\n",
            "***Epoch: 740, Loss:3.8954875469207764***\n",
            "***Epoch: 750, Loss:3.791247844696045***\n",
            "***Epoch: 760, Loss:3.6940512657165527***\n",
            "***Epoch: 770, Loss:3.6031653881073***\n",
            "***Epoch: 780, Loss:3.517930269241333***\n",
            "***Epoch: 790, Loss:3.4377682209014893***\n",
            "***Epoch: 800, Loss:3.3621768951416016***\n",
            "***Epoch: 810, Loss:3.2906768321990967***\n",
            "***Epoch: 820, Loss:3.222895860671997***\n",
            "***Epoch: 830, Loss:3.1584839820861816***\n",
            "***Epoch: 840, Loss:3.097118377685547***\n",
            "***Epoch: 850, Loss:3.0384936332702637***\n",
            "***Epoch: 860, Loss:2.9824137687683105***\n",
            "***Epoch: 870, Loss:2.928628444671631***\n",
            "***Epoch: 880, Loss:2.876950979232788***\n",
            "***Epoch: 890, Loss:2.827218770980835***\n",
            "***Epoch: 900, Loss:2.7792627811431885***\n",
            "***Epoch: 910, Loss:2.7329602241516113***\n",
            "***Epoch: 920, Loss:2.688192367553711***\n",
            "***Epoch: 930, Loss:2.6448304653167725***\n",
            "***Epoch: 940, Loss:2.602794647216797***\n",
            "***Epoch: 950, Loss:2.561997413635254***\n",
            "***Epoch: 960, Loss:2.522353410720825***\n",
            "***Epoch: 970, Loss:2.483806848526001***\n",
            "***Epoch: 980, Loss:2.4462730884552***\n",
            "***Epoch: 990, Loss:2.409717082977295***\n",
            "***Final***\n",
            "Targets: tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]], requires_grad=True)\n",
            "Predictions:tensor([[ 57.5258,  70.6373],\n",
            "        [ 80.8171, 100.7470],\n",
            "        [121.2774, 132.2619],\n",
            "        [ 22.0316,  37.0855],\n",
            "        [ 99.1471, 119.6003]], grad_fn=<AddBackward0>)\n",
            "Loss:2.3776040077209473\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch built-ins"
      ],
      "metadata": {
        "id": "moU0Cw5Avk3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "lr = 1e-5\n",
        "epochs = 1000\n",
        "batch_size = 5\n",
        "inputs = torch.tensor(inputs_orig, requires_grad=True)\n",
        "targets = torch.tensor(targets_orig, requires_grad=True)\n",
        "\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "\n",
        "model = nn.Linear(3, 2)\n",
        "print(model.weight)\n",
        "print(model.bias)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fcn = F.mse_loss\n",
        "\n",
        "for i in range(epochs):\n",
        "  for xb, yb in train_dl:\n",
        "    # compute predictions and loss\n",
        "    preds = model(xb)\n",
        "    loss = loss_fcn(preds, yb)\n",
        "\n",
        "    # compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # adjust the weights and reset the gradients\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "  if i > 0 and i % 10 == 0:\n",
        "    print('***Epoch: {}, Loss:{}***'.format(i, loss))\n",
        "  \n",
        "print('***Final***\\nTargets: {}\\nPredictions:{}\\nLoss:{}\\n'.format(targets, preds, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UGan959vp0Z",
        "outputId": "bbf4171c-ad00-4947-9c28-d4e1e0bb83a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1913, -0.3394, -0.2842],\n",
            "        [ 0.1763,  0.5359, -0.4976]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0097,  0.4950], requires_grad=True)\n",
            "***Epoch: 10, Loss:828.1764526367188***\n",
            "***Epoch: 20, Loss:606.9547119140625***\n",
            "***Epoch: 30, Loss:541.121337890625***\n",
            "***Epoch: 40, Loss:485.4707946777344***\n",
            "***Epoch: 50, Loss:436.34466552734375***\n",
            "***Epoch: 60, Loss:392.9202575683594***\n",
            "***Epoch: 70, Loss:354.5188903808594***\n",
            "***Epoch: 80, Loss:320.5444641113281***\n",
            "***Epoch: 90, Loss:290.47125244140625***\n",
            "***Epoch: 100, Loss:263.8368225097656***\n",
            "***Epoch: 110, Loss:240.23330688476562***\n",
            "***Epoch: 120, Loss:219.30184936523438***\n",
            "***Epoch: 130, Loss:200.72633361816406***\n",
            "***Epoch: 140, Loss:184.22805786132812***\n",
            "***Epoch: 150, Loss:169.56187438964844***\n",
            "***Epoch: 160, Loss:156.51165771484375***\n",
            "***Epoch: 170, Loss:144.88705444335938***\n",
            "***Epoch: 180, Loss:134.52023315429688***\n",
            "***Epoch: 190, Loss:125.2636489868164***\n",
            "***Epoch: 200, Loss:116.9870376586914***\n",
            "***Epoch: 210, Loss:109.57585144042969***\n",
            "***Epoch: 220, Loss:102.92915344238281***\n",
            "***Epoch: 230, Loss:96.95770263671875***\n",
            "***Epoch: 240, Loss:91.58323669433594***\n",
            "***Epoch: 250, Loss:86.73665618896484***\n",
            "***Epoch: 260, Loss:82.35712432861328***\n",
            "***Epoch: 270, Loss:78.39071655273438***\n",
            "***Epoch: 280, Loss:74.79048156738281***\n",
            "***Epoch: 290, Loss:71.51453399658203***\n",
            "***Epoch: 300, Loss:68.52609252929688***\n",
            "***Epoch: 310, Loss:65.79286193847656***\n",
            "***Epoch: 320, Loss:63.28618240356445***\n",
            "***Epoch: 330, Loss:60.980865478515625***\n",
            "***Epoch: 340, Loss:58.85478591918945***\n",
            "***Epoch: 350, Loss:56.88822555541992***\n",
            "***Epoch: 360, Loss:55.06397247314453***\n",
            "***Epoch: 370, Loss:53.366615295410156***\n",
            "***Epoch: 380, Loss:51.78285598754883***\n",
            "***Epoch: 390, Loss:50.30073165893555***\n",
            "***Epoch: 400, Loss:48.90970993041992***\n",
            "***Epoch: 410, Loss:47.60042190551758***\n",
            "***Epoch: 420, Loss:46.36483383178711***\n",
            "***Epoch: 430, Loss:45.195560455322266***\n",
            "***Epoch: 440, Loss:44.086341857910156***\n",
            "***Epoch: 450, Loss:43.03122329711914***\n",
            "***Epoch: 460, Loss:42.02544403076172***\n",
            "***Epoch: 470, Loss:41.0643424987793***\n",
            "***Epoch: 480, Loss:40.14412307739258***\n",
            "***Epoch: 490, Loss:39.26125717163086***\n",
            "***Epoch: 500, Loss:38.412532806396484***\n",
            "***Epoch: 510, Loss:37.59526824951172***\n",
            "***Epoch: 520, Loss:36.80709457397461***\n",
            "***Epoch: 530, Loss:36.04560852050781***\n",
            "***Epoch: 540, Loss:35.308937072753906***\n",
            "***Epoch: 550, Loss:34.59544372558594***\n",
            "***Epoch: 560, Loss:33.903404235839844***\n",
            "***Epoch: 570, Loss:33.23155975341797***\n",
            "***Epoch: 580, Loss:32.5786247253418***\n",
            "***Epoch: 590, Loss:31.943435668945312***\n",
            "***Epoch: 600, Loss:31.3250732421875***\n",
            "***Epoch: 610, Loss:30.72249984741211***\n",
            "***Epoch: 620, Loss:30.13498306274414***\n",
            "***Epoch: 630, Loss:29.561779022216797***\n",
            "***Epoch: 640, Loss:29.002182006835938***\n",
            "***Epoch: 650, Loss:28.455581665039062***\n",
            "***Epoch: 660, Loss:27.921483993530273***\n",
            "***Epoch: 670, Loss:27.399301528930664***\n",
            "***Epoch: 680, Loss:26.888626098632812***\n",
            "***Epoch: 690, Loss:26.388931274414062***\n",
            "***Epoch: 700, Loss:25.899959564208984***\n",
            "***Epoch: 710, Loss:25.42123031616211***\n",
            "***Epoch: 720, Loss:24.952457427978516***\n",
            "***Epoch: 730, Loss:24.49336051940918***\n",
            "***Epoch: 740, Loss:24.043537139892578***\n",
            "***Epoch: 750, Loss:23.60283851623535***\n",
            "***Epoch: 760, Loss:23.17092514038086***\n",
            "***Epoch: 770, Loss:22.747634887695312***\n",
            "***Epoch: 780, Loss:22.332603454589844***\n",
            "***Epoch: 790, Loss:21.92572784423828***\n",
            "***Epoch: 800, Loss:21.526817321777344***\n",
            "***Epoch: 810, Loss:21.13559341430664***\n",
            "***Epoch: 820, Loss:20.751964569091797***\n",
            "***Epoch: 830, Loss:20.37563705444336***\n",
            "***Epoch: 840, Loss:20.006567001342773***\n",
            "***Epoch: 850, Loss:19.64451789855957***\n",
            "***Epoch: 860, Loss:19.289310455322266***\n",
            "***Epoch: 870, Loss:18.940874099731445***\n",
            "***Epoch: 880, Loss:18.59899139404297***\n",
            "***Epoch: 890, Loss:18.263614654541016***\n",
            "***Epoch: 900, Loss:17.934558868408203***\n",
            "***Epoch: 910, Loss:17.611621856689453***\n",
            "***Epoch: 920, Loss:17.294824600219727***\n",
            "***Epoch: 930, Loss:16.98395538330078***\n",
            "***Epoch: 940, Loss:16.678865432739258***\n",
            "***Epoch: 950, Loss:16.379501342773438***\n",
            "***Epoch: 960, Loss:16.0856990814209***\n",
            "***Epoch: 970, Loss:15.79743766784668***\n",
            "***Epoch: 980, Loss:15.514520645141602***\n",
            "***Epoch: 990, Loss:15.236856460571289***\n",
            "***Final***\n",
            "Targets: tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]], requires_grad=True)\n",
            "Predictions:tensor([[ 57.3688,  70.8131],\n",
            "        [ 98.6103, 111.9398],\n",
            "        [121.9105, 140.1178],\n",
            "        [ 80.6482,  97.1667],\n",
            "        [ 22.1038,  39.0737]], grad_fn=<AddmmBackward0>)\n",
            "Loss:14.991430282592773\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy from scratch"
      ],
      "metadata": {
        "id": "Z-m9gFutvq1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3-Bm4E3xvtQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}